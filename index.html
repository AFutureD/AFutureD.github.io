<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="AFuture&#39;s Blog">
<meta property="og:url" content="http://blog.afuture.me/index.html">
<meta property="og:site_name" content="AFuture&#39;s Blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AFuture&#39;s Blog">






  <link rel="canonical" href="http://blog.afuture.me/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>AFuture's Blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AFuture's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/07/数列极限-题一/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/07/数列极限-题一/" itemprop="url">
                  数列极限 题一
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-07 14:35:25 / 修改时间：15:11:31" itemprop="dateCreated datePublished" datetime="2018-07-07T14:35:25+08:00">2018-07-07</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[题]：<br>设 $-\pi &lt; x &lt; \pi $ ,则 $\underbrace{sinsinsinsinsin}_{n个}x$ 是否存在？<br>若存在，证明并求之；<br>若不存在，说明理由。</p>
<p>[解]：<br>    令 $ M_1 = sinx, M_2 = sin(sinx) \cdots M_n=sin(M_{n-1})$,<br>    证单调性。$M_n$ 为奇函数，则考虑 $x \in [0,\pi)$ 。因为 $M_1 \in (-1,1) $，且 $sinx$ 单调递减，所以 $ 0&lt; M_2 &lt; M_1 &lt; 1$ 。<br>    则明显有 $ 0 &lt; M_n &lt; M_{n-1} \cdots M_2 &lt; M_1 &lt; 1 \tag{1} $<br>    即，$M_n$ 单调递减。<br>    证有界。$sinx$ 天生有界，由(1)知 $ 0 &lt; M_n &lt; 1 $。<br>    故 $\underset{n \to \infty}{lim}{M_n}$ 存在。设 $\underset{n \to \infty}{lim}{M_n} = A $，令 $n \to \infty $, 由 $M_n=sin(M_{n-1})$ ，得 $ A = sinA$,解得 $A = 0$。<br>    同理，当 $x \in （-\pi,0] $ 时，$ A  = 0 $ 。则极限为 0 。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/05/求极限-题二/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/05/求极限-题二/" itemprop="url">
                  求极限 题二
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-05 14:21:41 / 修改时间：15:31:51" itemprop="dateCreated datePublished" datetime="2018-07-05T14:21:41+08:00">2018-07-05</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>求：</p>
<p>$$<br>\lim_{x \to \infty} \frac{tan^3{\frac{1}{x}}-arctan{\frac{2}{\sqrt[3]{x^4}}}}{sin{\frac{3}{x^3}}-arcsin{\frac{2019}{x}}}<br>$$</p>
<p>[分析]：<br>$$<br>\begin{align}<br> because : \<br> &amp; \lim_{x \to \infty} {\frac{arctan{\frac{2}{\sqrt[3]{x^4}}}}{tan^3{\frac{1}{x}}}} \overset{洛必达}{\Longrightarrow} \frac{\frac{2}{\sqrt[3]{x^4}}}{\frac{1}{x^3}} = \infty \\<br>therefore :\<br>&amp; arctan{\frac{2}{\sqrt[3]{x^4}}} \ggg tan^3{\frac{1}{x}} \\<br>in\ a\ similar\ way: \\<br>&amp; arcsin{\frac{2019}{x}} \ggg sin{\frac{3}{x^3}} \\<br>therfore:\<br>&amp; original\ formula = \frac{arctan{\frac{2}{\sqrt[3]{x^4}}}}{arcsin{\frac{2019}{x}}} = 0<br>\end{align}<br>$$</p>
<p>[注]：<br>高阶无穷小可舍去。原因：0.01 + 0.000001 ～ 0.01 ，因此，可以舍去那个极小的值。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/Normalization-and-Regularization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/Normalization-and-Regularization/" itemprop="url">
                  Normalization and Regularization
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:51:09 / 修改时间：17:02:33" itemprop="dateCreated datePublished" datetime="2018-07-03T14:51:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>Normalization 就是把数据进行前处理，从而使数值都落入到统一的数值范围，从而在建模过程中，各个特征量没差别对待。normalization 一般是把数据限定在需要的范围，比如一般都是 ${0,1}$ ，从而消除了数据量纲对建模的影响。并且对基于 gradient descent 的算法友好，能加快训练速度，促进算法的收敛。</p>
<p>注：Standardization 是 Normalization 的一种特殊情况，它对数据进行正态化处理，使数据的平均值为1，方差为0。</p>
<p>Regularization 是在 cost function 里面加惩罚项，增加建模的模糊性，从而把捕捉到的趋势从局部细微趋势，调整到整体大概趋势。虽然一定程度上的放宽了建模要求，但是能有效防止过拟合（over-fitting）的问题。  </p>
<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>Normalization 的手段很多，主要有：</p>
<ol>
<li>min-max normalization:  $x’=\frac{x-min}{max-min}$ .</li>
<li>logarithmic transformations：$x’ = \frac{\log{(x)}} {\log{(x_{max})}}$ .</li>
<li>arctan function：$x’= arctan(x)$ .</li>
<li>zero mean normalization：$x’=\frac{x-\mu}{\sigma}$</li>
</ol>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>Regularization 主要是处理过拟合的情况，它对某些特征值进行处罚，简单来说就是降低重要性。</p>
<p>以 Liner Regression 为例，在算法中，我们的步骤为：</p>
<p>cost function：<br>$$<br>J(\theta)=\frac{1}{2m}\left[ \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)} )+\lambda\sum^n_{j=1}\theta^2_j \right]<br>$$<br>求导完后有梯度下降迭代式，其中 $x_0$ 始终为 1 ，不参与迭代：</p>
<p>$$<br>\begin{align}<br>&amp;\large{重复直到收敛}\{\\<br>&amp; \quad \quad \theta_0 := \theta_0-\alpha \cdot \frac{1}{m}\sum^{m}{i=1}(h{\theta}(x^{(i)})-(y^{(i)})x^{(i)}_0 \\<br>&amp; \quad \quad \theta_j := \theta_j -\alpha \cdot \left[ \left( \frac{1}{m}\sum^{m}{i=1}(h{\theta}(x^{(i)})-(y^{(i)})x^{(i)}_j \right) +\frac{\lambda}{m}\theta_j\right]\\<br>&amp;\}<br>\end{align}<br>$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/Decision-Tree/" itemprop="url">
                  决策树（decision tree）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:50:09 / 修改时间：17:14:28" itemprop="dateCreated datePublished" datetime="2018-07-03T14:50:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>决策树本事上是一棵树，树的根节点包含了样本全集，从根出发到叶节点的路径对应了一个判定测试序列，而每个内部节点包含的样本集合则根据相应属性的测试结果而划分到子节点中。</p>
<p>注：决策树在 Coursera 和 Stanford 的公开课中并未提到，相关知识完全通过《机器学习》和互联网获得。</p>
<h2 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h2><p>讲基本过程之前，先来搞懂“先验概率”、“后验概率”、“似然函数”。</p>
<p>先验概率：就是常识、经验或者统计方法所透露出的“因”的概率，$p$(原因)，就是先验概率。</p>
<p>后验概率：这种先知道结果，然后由结果估计原因的概率分布，$p$(原因|结果)，就是后验概率。</p>
<p>似然函数：这种先确定原因，根据原因来估计结果的固有性质的可能性（likelihood），是对固有性质的拟合程度，$L$(结果|原因)，就是似然估计。</p>
<p>于是有了贝叶斯公式：</p>
<p>$$<br>p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}<br>$$</p>
<p>$x$ ：观察得到的数据（结果）</p>
<p>$\theta$ ：决定数据分布的参数（原因）</p>
<p>$p(\theta|x)$ ：后验概率（posterior probability）</p>
<p>$p(\theta)$ ：先验概率（prior probability）</p>
<p>$p(x|\theta)​$ ：似然估计（likelihood）</p>
<p>$p(x)$ ：evidence</p>
<p>懂了上述概念，接着讲讲基本流程，训练集 $D={(x_1,y_1),(x_2,y_2), \cdots , (x_m,y_m)}$; 属性集 $A={a_1,a_2, \cdots,a_d}$：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">函数 TreeGenerate(D,A)</span><br><span class="line">生成节点node:</span><br><span class="line">if D 中的样本全属于同一类别 C then</span><br><span class="line">    将node标记为C类叶子节点；return</span><br><span class="line">end if</span><br><span class="line">if A = ϕ OR D 中样本在 A 上取值相同 then</span><br><span class="line">    将 node 标记为叶节点，其类别标记为 D 中样本数最多的类；return</span><br><span class="line">end if</span><br><span class="line">从A中选则最优划分属性a*;</span><br><span class="line">for a* 的每一个值a*(v) do</span><br><span class="line">    为node生成一个分支；令Dv表示D中在a*上取值为a*(v)的样本子集；</span><br><span class="line">    if Dv 为空 then</span><br><span class="line">        将分支节点标记为叶节点，其类别标记为D中样本最多的类；return</span><br><span class="line">    else</span><br><span class="line">        以Tree(Dv,A\&#123;a*&#125;)为分支节点</span><br><span class="line">    end if</span><br><span class="line">end for</span><br></pre></td></tr></table></figure>
<p>决策数的生成是一个递归的过程，在决策树基本算法中有三种情况会导致递归返回：</p>
<ol>
<li>当前节点包含的样本种类属于同一类别，无需划分</li>
<li>当前样本属性集为空，或者所有样本在所有属性上的取值相同，无法划分</li>
<li>当前节点包含的样本集合为空，不能划分</li>
</ol>
<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><p>上面的基本过程的的本质是构建一课树，核心是划分最优属性 $a_*$ ,且希望决策树的分支节点所包含的样本尽可能的属于同一类别，即节点的“纯度”尽可能高。</p>
<p>划分选择有多种算法，如：ID3、C4.5。</p>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><p>在一开始 我们提一下熵。熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。<br>信息熵(information entropy)是度量样本集合纯度最常用的一种指标，假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k(k=1,2,\cdots , |\mathcal{Y}|)$ ，则 $D$ 的信息熵定义为：<br>$$<br>Ent(D)=-\sum^{|\cal{Y}|}_{k=1}p_k\log_2p_k .<br>$$<br>Ent(D)的值越小，则D的纯度越高。</p>
<p>为了找出“纯度最大”的属性，我们引入“信息增熵”，简单来说就是“信息熵”与“条件熵”的差值，即信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度；表明了此条件的重要性。再给出“信息增熵”的定义：</p>
<p>$$<br>Gain(D,a)=Ent(D)-\sum^{V}_{v=1}\frac{|D^v|}{|D|}Ent(D^v)<br>$$</p>
<p>所以：有了</p>
<p>$$<br>\DeclareMathOperator*{\argmax}{arg\,max}<br>a_* = \mathop{\argmax}_{a \in A}{Gain(D,a)}<br>$$</p>
<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><p>为了解决信息增熵准则对可取数目较多的属性有所偏好，著名的 C4.5 决策树算法 [Quinlan,1993] 使用“增益率” （gain ration）来选择最优划分属性。</p>
<p>增益率定义为：<br>$$<br>Gain _ratio(D,a)=\frac{Gain(D,a)}{IV(a)}<br>$$<br>其中：<br>$$<br>\rm{IV}(a)=-\sum^V_{v=1}\frac{|\it{D}^{\it{v}}|}{|\it{D}|}\log_2{\frac{|\it{D}^{\it{v}}|}{|\it{D}|}}<br>$$<br>IV(a) 称为属性 a 的固有值，属性 a 的可能取值数目越多，则 IV(a) 的值通常会越大。增益率准则对取值数目较少的属性有所偏好。</p>
<p>因此，使用启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p> CART树(Classification and Regression Tree)使用基尼指数来选择属性的划分，通过基尼值来度量数据集的纯度<br>基尼值： </p>
<p>$$<br>\begin{align}<br>Gini(D)&amp;=\sum^{|\cal{Y}|}_{k=1}\sum_{k’\neq k}p_k p_{k’}\\<br>&amp;=1-\sum^{|\cal{Y}|}_{k=1}p_k^2<br>\end{align}<br>$$</p>
<p>反映了从数据集 $D$ 中取出两个样本，不为同一种类的概率，因此 Gini(D) 越小，数据集的纯度越高。 </p>
<p>转化成与“信息增熵”相似的定义：<br>$$<br>Gini\_index(D,a)=\sum^{V}_{v=1}\frac{|D^v|}{|D|}Gini(D^v)<br>$$<br>于是最优划分属性为：<br>$$<br>\DeclareMathOperator*{\argmin}{arg\,min}<br>a_* = \mathop{\argmin}_{a \in A}{Gini\_index(D,a)}<br>$$</p>
<h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>预剪枝是在决策树生成的过程中，对每个结点在划分前先进行预估，若当前结点的划分不能使决策树泛化性能提升，则停止划分并将当前结点标记为叶节点。</p>
<p>即对比当前验证集精度与生成子决策树后的精度</p>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>后剪枝是先从训练集中生成一颗完整的决策树，然后自底向上的对非叶子结点进行考察，若将改结点对应的子树替换为叶子结点能提高泛化能力，则进行替换。</p>
<p>即，对每一分支，对比当前精度与去掉决策子树的精度。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>后剪枝决策树比预剪枝决策树保留了更多的分支，一般情况下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树，但后剪枝过程是生成完全决策树之后进行的，并且要自底向上地对书中的所有非叶子节点进行逐一考察，因此其训练时间开销比末剪纸决策树和预剪枝决策树都要大得多。</p>
<h2 id="连续值缺失值处理"><a href="#连续值缺失值处理" class="headerlink" title="连续值缺失值处理"></a>连续值缺失值处理</h2><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><p>由于连续属性的可能取值不再有限，因此不能直接根据连续属性的可能取值进行划分。我们可以使用离散化技术对其进行处理。<br>二分法：<br>给定样本集 $D$ ，和连续属性 $a$ ，$a$ 在 $D$ 上出现了 $n$ 个不同的取值，<br>将其排序 ${a_1,a_2,\cdots,a_n}$ ，然后基于划分点 $t$ 将样本划分为 $D−t$ 和 $D+t$ ，$D−t$ 包括了在属性 $a$ 上取值小于 $t$ 的样本，$D+t$ 反之。即<br>$$<br>T_a={\frac{a_i+a_{i+1}}{2}|1 \leq i \leq n−1}<br>$$<br>通常我们取区间 $[a^i,a^{a+1})$ 的中位点作为候选划分点。</p>
<p>然后有：<br>$$<br>\DeclareMathOperator*{\argmin}{arg\,min}<br>\begin{align}<br>Gain(D,a)&amp;=\max \limits_{t \in T_a} Gain(D,a,t)\\<br>&amp;=\max \limits_{t \in T_a}  Ent(D)-\sum_{\lambda in {-,+}} \frac{|D^{\lambda}_t|}{|D|}Ent(D^{\lambda}_t)<br>\end{align}<br>$$</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>现实任务中常常会遇到不完整的样本，也就是样本中的某些属性值缺失的情况，最简单的方法是直接去除缺失的数据，但是这是对数据信息的极大浪费，我们可以考虑下利用有缺失属性值的样本来进行学习。<br>需解决的问题：</p>
<ol>
<li>在属性值缺失的情况下进行划分属性的选择</li>
<li>给定划分属性，若样本在该属性上的值缺失，如何进行划分</li>
</ol>
<p>给定训练集 $D$ ,属性集 $a$ ,令 $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集。</p>
<p>对问题一：可依据 $\tilde{D}$ 来判断属性的优劣。假定属性 $a$ 有 $V$ 个可取的值 ${a^1,a^2, \cdots , a^V}$ ，令 $\tilde{D}^v$ 表示 $\tilde{D}$ 中在属性 $a$ 上的取值为 $a^v$ 的样本集， $\tilde{D}_k$  表示 $\tilde{D}$ 中属于第 $k$ 类的样本子集。</p>
<p>显然有 $ \tilde{D} = \bigcup^{|\cal{Y}|}_{k=1}\tilde{D}_k,\tilde{D} = \bigcup^{|V|}_{v=1}\tilde{D} ^v$ 。</p>
<p>假定给每个样本一个权重 $w_x$ ，并定义：</p>
<p>$$<br>\begin{align}<br>\rho&amp;=\frac{\sum_{x \in  \tilde{D} }w_x}{\sum_{x \in D}w_x}\\<br>\tilde{p}_k&amp; = \frac{\sum_{x \in  \tilde{D}_k }w_x}{\sum_{x \in D}w_x} &amp;(1 \leq k \leq |\cal{Y}|)\\<br>\tilde{r}_v&amp; = \frac{\sum_{x \in  \tilde{D}^v }w_x}{\sum_{x \in D}w_x} &amp;(1 \leq k \leq |\cal{Y}|)<br>\end{align}<br>$$</p>
<p>$\rho$ ：在属性 $a$ 中 ，无缺失值所占样本比例；</p>
<p>$\tilde{p}_k$ ：无缺失值样本中第 $k$ 类所占的比例；</p>
<p>$\tilde{r}_v$ ：无缺失值样本中在属性 $a$ 上的取值 $a^v$ 的样本所占比例 </p>
<p>显然有，$\sum^{ |\cal{Y}|}_{k=1} \tilde{p}_k=1,\sum^{ |V|}_{c=1} \tilde{r}_v=1$ 。</p>
<p>于是可以推广信息增益的计算式为：<br>$$<br>\begin{align}<br>Gain(D,a)&amp;=\rho \times Gain(\tilde{D},a)\\<br>&amp;=\rho \times \left(  Ent \left( \tilde{D} \right)-\sum^{V}_{v=1}\tilde{r}_vEnt(\tilde{D} ^v) \right)<br>\end{align}<br>$$<br>其中：<br>$$<br>Ent(\tilde{D}) = -\sum^{|\cal{Y}|}_{k=1}\tilde{p}_k \log_2\tilde{p}_k<br>$$<br>对问题二，样本 $x$ 在属性 $a$ 上的取值缺失，则将 $x$ 划分到所有叶子结点中，将权值变为 $\tilde{r}_v \cdot w_x$ 意思将同一个样本以不同的概率划入到同的子结点中去。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/Logistic-Regression/" itemprop="url">
                  Logistic Regression
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:49:09 / 修改时间：17:00:41" itemprop="dateCreated datePublished" datetime="2018-07-03T14:49:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>在 Liner Regression 中 $\pmb{y}$ 是一个连续值，那么当我们解决一个二分类问题时 $\pmb{y}$ 则是一个离散值，只有两个取值（ $0$ 或 $1$ ），这时可以通过广义线性模型来解决。通过广义线性模型，我们找到一个单调可微函数将分类任务的标记 $\pmb{y}$ 与线性模型的预测值联系起来。最后我们找到 Logistic Function 来作为线性模型。</p>
<h2 id="广义线性模型（Generalized-Linear-Models-）"><a href="#广义线性模型（Generalized-Linear-Models-）" class="headerlink" title="广义线性模型（Generalized Linear Models ）"></a>广义线性模型（Generalized Linear Models ）</h2><p>在笔记中，我先把广义线性模型梳理一遍，这有助于我更好得学习机器学习。</p>
<p>当然如果仅仅是为了学习 Logistic Function 那么可以先看看后面的内容，之后再看这部分内容。</p>
<p>在回归学习中，我们的函数都类似于$f(\pmb{y}|\pmb{x}); \pmb{\theta} \sim \mathcal{N}(\mu,\sigma^2)$ 或者在之后讲的二分类函数 $f(\pmb{y}|\pmb{x}); \pmb{\theta} \sim \rm{Bernoulli}(\phi)$ ，这里的 $\mu$ 和 $\phi$ 都分别是 $x$ 和 $\theta$ 的某种函数（ $\sigma$ 与分布无关 ）。其实，有一种更广泛的模型，这两种模型都是它的特例，这种更广泛的模型叫做广义线性模型。</p>
<blockquote>
<p>  在广义线性模型中(GLM), 对于每个独立参数的 $\pmb{y}$ ，假设通过一个指数族产生。这就是说，对于均值 $\mu$ , 和独立变量 $\pmb{x}$，有：<br>  $$<br>  E(\pmb{y})=\pmb{\mu}=g^{-1}(\pmb{\theta}^{\mathit{T}} \pmb{x})<br>  $$<br>  $E(\pmb{y})$ 是 $\pmb{y}$ 的期望；$\pmb{\theta}^{\mathit{T}} \pmb{x}$  是一个线性估计; $g$ 是链接函数。</p>
</blockquote>
<p>关于广义线性模型更多的知识请前往 <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Model_components" target="_blank" rel="noopener">Wikipedia</a> 。</p>
<h3 id="指数族（The-Exponential-Family-）"><a href="#指数族（The-Exponential-Family-）" class="headerlink" title="指数族（The Exponential Family ）"></a>指数族（The Exponential Family ）</h3><p>在学习广义线性模型之前，我们要先定义一下指数族分布（exponential family distributions）。如果一个分布能用下面的方式来写出来，我们就说这类分布属于指数族：<br>$$<br>p(y;\eta)=b(y)\rm{exp}(\eta^TT(y)-a(\eta))<br>$$<br>上面的式子中，</p>
<p>$\eta$ ：该分布的自然参数（natural parameter，也叫典范参数 canonical parameter）；</p>
<p>$T(y)​$ ：充分统计量（sufficient statistic），我们目前用的这些分布中通常 $T(y) = y​$ ；</p>
<p>$a(\eta)$ ：一个<strong>对数分割函数（log partition function）</strong>；</p>
<p>$e^{−a(\eta)} $ ：这个量本质上扮演了归一化常数（normalization constant）的角色，也就是确保分布的 $p(y;\eta)$ 的总和等于1。</p>
<p>对 给定的 $T$ , $a$ 和 $b$ 就定义了一个以 $\eta$ 为参数的分布族（family，或者叫集 set）；通过改变 $\eta$ ，我们就能得到这个分布族中的不同分布。 </p>
<p>现在咱们看到的伯努利（Bernoulli）分布和高斯（Gaussian）分布就都属于指数分布族。伯努利分布的均值是 $\phi$ ，也写作 $\rm{Bernoulli}(\phi)$ ，确定的分布是 $y \in{0,1}$，因此有 $p(y=1;\phi)=\phi;p(y=0;\phi)=1-\phi$。这时候只要修改 $\phi$ ，就能得到一系列不同均值的伯努利分布了。现在我们展示的通过修改 $\phi$ ,而得到的这种伯努利分布，就属于指数分布族；也就是说，只要给定一组 $T$ , $a$ 和 $b$ ，就可以用上面的等式来确定一组特定的伯努利分布了。</p>
<p>伯努利分布通过广义线性模型可以这样写：<br>$$<br>\begin{align}<br>p(y;\phi) &amp;= \phi^y(1-\phi)^{1-y}\\<br>&amp;=\rm{exp}(\it{y}\rm{}\log{\phi}+(1-y)\log{(1-\phi)})\\<br>&amp;=\rm{exp}\left(\left(\log{\left(\frac{\phi}{1-\phi}\right)}\right)\it{y}\rm{}+\log{(1-\phi)}\right)<br>\end{align}<br>$$</p>
<p>因此，给出了自然参数（natural parameter）$\eta=\log{\left(\frac{\phi}{1-\phi}\right)}$ 。 有趣的是，如果我们翻转这个定义，通过 $\phi$ 表示 $\eta$ 就会得到 $\phi=\frac{1}{1+e^{-\eta}}$ 。这正好就是我们之后在 Logistic Function 中会见到的 S 型函数（sigmoid function）！ 在我们把逻辑回归作为一种广义线性模型（GLM）的时候还会遇到伯努利分布以如下情况表示。<br>$$<br>\begin{align}<br>T(y) &amp;= y\\<br>a(\eta)&amp;=-\log{(1-\phi)}\\<br>&amp;=log(1+e^\eta)\\<br>b(y)&amp;=1<br>\end{align}<br>$$<br>接下来就看看高斯分布。在推导线性回归的时候， $\sigma^2$ 的值对我们最终选择的 $\theta $ 和 $h_\theta(x)$ 都没有影响。所以我们可以给 $\sigma^2$ 取一个任意值。为了简化推导过程，就令 $\sigma^2 = 1$ 。然后就有了下面的等式：<br>$$<br>\newcommand{\itm}[1]{\mathcal {#1}\rm}<br>\newcommand{\bgroup}[1]{\left( {#1}\right)}<br>\begin{align}<br>p(y;\mu)&amp;=\frac{1}{\sqrt{2\pi}}\rm{exp}\left(-\frac{1}{2}(\itm{y}\rm{}-\mu)\right)\\<br>&amp;=\frac{1}{\sqrt{2\pi}}\rm{exp}\bgroup{-\frac{1}{2}\itm{y}^2}\cdot \rm{exp}\bgroup{\mu\itm{y}-\frac{1}{2}\mu^2}<br>\end{align}<br>$$</p>
<p>注：如果我们把 $\sigma^2$ 作为一个变量，高斯分布就也可以表达成指数分布的形式，其中 $\eta \in \mathbb{R}^2$ 就是一个同时依赖  $\mu$  和 $\sigma$ 的二维向量。然而，对于广义线性模型GLMs方面的用途，参数  $\sigma^2$ 就也可以看成是对指数分布族的更泛化的定义：$p(y;\eta,\tau)=b(a,\tau)\rm{exp}((\eta^T\it{T}(\itm{y})-\mathcal{a}(\eta))/\mathcal{c}(\tau))$ 。这里面的 $\tau$ 叫做分散度参数（dispersion parameter），对于高斯分布，$c(\tau)=\sigma^2$ ；不过上文中已经进行了简化，所以就不对各种需要考虑的情况进行更为泛化的定义了。</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><h3 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h3><p>从最基本开始，我们不使用广义线性模型，对于二分类问题，输出标记 $\pmb{y} \in {0,1} $ ，于是我们使用线性回归最基本的模型 $z=  \pmb{\theta}^{\mathit{T}} \pmb{x}$ 来预测 $\pmb{y}$ ，即我们将 $z$ 转化为 $0/1$ 值。最理想的是 “单位越阶函数”（unit-step function）：<br>$$<br>\newcommand{\itm}[1]{\it {#1}\rm}<br>\newcommand{\bgroup}[1]{\left( {#1}\right)}<br>y=\left\{<br>\begin{aligned}<br>0,&amp; &amp;z&lt;0; \\<br>0.5,&amp; &amp;z=0; \\<br>1, &amp; &amp;z&gt;0;<br>\end{aligned}<br>\right.<br>$$</p>
<p>但是，很明显，此函数不是很完美，于是，我们找到了一个“替代函数”来近似这个“单位跃阶函数”，并希望它单调可微，对数几率函数（Logistic Function）便满足这样一个条件：<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fl5zm1lftxj30jc0feq3p.jpg" alt=""></p>
<p>由图你能直观得看到，当 $z\to+\infty$ 的时候 $g(z)$ 趋向于 $1$ ，而当 $z\to-\infty$ 时 $g(z)$ 趋向于 $0$ 。</p>
<p>其实 $g(z)$ 也是 $h_\theta(x)$ ，且，像最开始一样，我们规定 $X_0 =1$ ，于是有：$\pmb{\theta}^Tx=\theta_0+\sum^{n}_{j=1}\theta_jx_j$ </p>
<p>现在我们看看 $g’(z)$ 的特性：<br>$$<br>\begin{align}<br>g’(z) &amp;= \frac{d}{dz}\frac{1}{1+e^{-z}}\\<br>&amp;=\frac{1}{1+e^{-z}}(e^{-z})\\<br>&amp;=\frac{1}{1+e^{-z}}\left(1-\frac{1}{1+e^{-z}}\right)\\<br>&amp;=g(z)(1-g(z)).<br>\end{align}<br>$$<br>接着我们通过对 $h_\theta(x)$ 进行假设，得到：<br>$$<br>p(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}<br>$$<br>然后就能写出似然函数：<br>$$<br>L(\pmb{\theta}) = p(\pmb{y}|X;\pmb{\theta})<br>$$<br>又与之前一样写出 $L(\theta)$ 的对数函数 $\ell(\pmb{\theta})$ 以方便计算。</p>
<p>于是有 cost function：<br>$$<br>J(\theta) = -\frac{1}{m}\ell(\pmb{\theta})<br>$$<br>然后目标就是：<br>$$<br>\DeclareMathOperator*{\argmin}{arg\,min}<br>\pmb{\theta} = \mathop{\argmin}_{}{J(\theta)}<br>$$</p>
<h2 id="pmb-theta-的求法"><a href="#pmb-theta-的求法" class="headerlink" title="$\pmb{\theta}$ 的求法"></a>$\pmb{\theta}$ 的求法</h2><p>我们从最开始得到的假设函数讲起。如何得到它呢？</p>
<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>我们首先假设：<br>$$<br>P(y=1|x;\theta)=h_\theta(x)\\<br>P(y=0|x;\theta)=1-h_\theta(x)<br>$$<br>于是，更简单的写法就是：<br>$$<br>p(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}<br>$$</p>
<h3 id="似然函数（likelihood-function）"><a href="#似然函数（likelihood-function）" class="headerlink" title="似然函数（likelihood function）"></a>似然函数（likelihood function）</h3><p>假设 $m$ 个训练样本都是各自独立的，那么就可以按如下的方式来写带参数的似然函数：<br>$$<br>\begin{align}<br>L(\pmb{\theta}) &amp;= p(\pmb{y}|X;\pmb{\theta})\\<br>&amp;=\prod^m_{i=1}p(y^{(i)}|x^{(i)};\pmb{\theta})  &amp;将不同的样本的概率相乘\\<br>&amp;=\prod^m_{i=1}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}<br>\end{align}<br>$$<br>取对数更容易计算：<br>$$<br>\begin{align}<br>\ell(\pmb{\theta})&amp;=\log{L(\theta)}\\<br>&amp;=\sum^{m}_{i=1}y^{i}\log{h(x^{(i)})}+ (1-y^{i})\log{(1-h(x^{(i)}))}<br>\end{align}<br>$$</p>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>极大似然函数中，为了求得最优的 $\pmb{\theta}$ ，就是让 $\ell(\pmb{\theta})$ 尽可能得大，于是在 cost function 中，我们加入系数 $-\frac{1}{m}$ ，于是得到了：<br>$$<br>J(\theta) = -\frac{1}{m}\sum^{m}_{i=1}y^{i}\log{h(x^{(i)})}+ (1-y^{i})\log{(1-h(x^{(i)}))}<br>$$</p>
<h3 id="梯度下降法（gradient-descent）"><a href="#梯度下降法（gradient-descent）" class="headerlink" title="梯度下降法（gradient  descent）"></a>梯度下降法（gradient  descent）</h3><p>其实，可以直接对 $\ell(\pmb{\theta})$ 做梯度上升求得 $\pmb{\theta}$ 。</p>
<p>按照向量的形式，我们对 $\theta$ 的更新可以写成：<br>$$<br>\pmb{\theta}:=\pmb{\theta}-\alpha \cdot \nabla_{\theta} J(\pmb{\theta})<br>$$<br>找到最优的第一步是对 $J(\pmb{\theta})$ 求导，我们从一个样本开始：<br>$$<br>\begin{align}<br>\frac{\partial}{\partial \theta_j}J(\theta)&amp;=-\frac{1}{m}\ell(\theta)\\<br>&amp;=-\frac{1}{m}\left( {y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}}\right)\frac{\partial}{\partial\theta_j}g(\theta^Tx)\\<br>&amp;=-\frac{1}{m}\left( {y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}}\right)g(\theta^Tx)(1-g(\theta^Tx))\\<br>&amp;=-\frac{1}{m}\left(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx)\right)x_j\\<br>&amp;=\frac{1}{m}(h_\theta(x)-y)x_j<br>\end{align}<br>$$<br>其中，用到了上面提到的  $g’(z)$ 的特性，即 $\frac{\partial}{\partial\theta_j}g(\theta^Tx)=g(\theta^Tx)(1-g(\theta^Tx))$ ，然后梯度上升就简单写为：<br>$$<br>\pmb{\theta}:=\pmb{\theta}-\alpha \cdot \frac{1}{m} (y-h_\theta(x))x_j<br>$$<br>然后，再扩展为一个训练集：<br>$$<br>\begin{align}<br>&amp;\large{重复直到收敛}\{\\<br>&amp; \quad \quad \theta_j := \theta_j +\alpha \cdot \frac{1}{m} \sum^{m}_{i=1}(y^{(i)}-h_{\theta}(x^{(i)}))x_j\\<br> &amp;\}<br>\end{align}<br>$$<br>有趣的是，这个式子正好与线性回归看上去一样，但是这实际上并不相同，原因是，我们对于  $h_\theta(x)$  的定义不同。但为什么相似呢？深层次的原因在于 <strong>广义线性模型</strong> 。</p>
<h3 id="L-theta-最大的其它算法"><a href="#L-theta-最大的其它算法" class="headerlink" title="$L(\theta)$ 最大的其它算法"></a>$L(\theta)$ 最大的其它算法</h3><p>下面这个方法更好，但是数学难度较高，其基本方法是“求方程零点的牛顿法”。</p>
<p>具体讲讲。假如我们有一个从实数到实数映射的函数 $\it{f}:\rm{R} \mapsto \rm{R}$，然后要找到一个 $\theta$ ，来满足 $\it{f}\,\rm{ }(\theta)=0$，其中$\theta \in R$是一个实数。牛顿法就是对 $\theta$ 进行如下的更新：<br>$$<br>\theta:=\theta-\frac{f(\theta)}{f’(\theta)}<br>$$<br>对这个公式的简单解释是：通过一条逼近曲线的直线（切线）不断迭代来找到零点。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fl66kyi55pj30co0b6q35.jpg" alt=""></p>
<p>对于上述的图，在 A 的切线方程为：$y_A = f(x_A)+f’(x_A)(x-x_A)$ 。</p>
<p>为了求下一个迭代点 B ，则有 $y_B = 0 = f(x_A)+f’(x_A)(x_B-x_A)$ ，</p>
<p>即：<br>$$<br>\begin{align}<br>&amp;\Longrightarrow x_B=x_a-\frac{f(x_A)}{f’(x_A)}\\<br>&amp;\Longrightarrow x_{n+1}=x_{n}-\frac{f(x_n)}{f’(x_n)}\\<br>&amp;\Longrightarrow \theta:=\theta-\frac{f(\theta)}{f’(\theta)}<br>\end{align}<br>$$<br>用这个办法，我们求解 $\ell(\theta)$ 中 $\theta$ 的最优取值，即算 $\ell(\theta) = 0$ 的解，即可以通过以下迭代式求得：<br>$$<br>\theta:=\theta-\frac{\ell’(\theta)}{\ell’’(\theta)}<br>$$<br>对于向量 $\pmb{\theta}$ 的求解，我们扩展牛顿法到多维的情况，叫做牛顿-拉普森法（Newton-Raphson method），如下：<br>$$<br>\pmb{\theta} := \pmb{\theta}-H^{-1}\nabla_{\theta} \ell(\pmb{\theta})<br>$$<br>其中 $\nabla_{\theta} \ell(\pmb{\theta})$ 是 $\ell(\pmb{\theta})$ 对 $\theta$ 的偏导。$H$ 是一个 $n\times n$ 矩阵（考虑 $\theta_0$ 的话是 $(n+1)\times (n+1)$  ），也可叫做 Hessian，具体定义为：<br>$$<br>H_{ij} = \frac{\partial^2\ell(\theta)}{\partial\theta_i\partial\theta_j}<br>$$<br>注意，当 $n$ 小的时候牛顿法的速度明显更快，但是当 $n$ 较大时，由于需要处理 Hession 矩阵，时间开销急剧增加。</p>
<h2 id="构建广义线性模型"><a href="#构建广义线性模型" class="headerlink" title="构建广义线性模型"></a>构建广义线性模型</h2><p>设想你要构建一个模型，来估计在给定的某个小时内来到你商店的顾客人数（或者是你的网站的页面访问次数），基于某些确定的特征 $X$ ，例如商店的促销、最近的广告、天气、今天周几啊等等。我们已经知道泊松分布（Poisson distribution）通常能适合用来对访客数目进行建模。知道了这个之后，怎么来建立一个模型来解决咱们这个具体问题呢？非常幸运的是，泊松分布是属于指数分布族的一个分部，所以我们可以使用一个广义线性模型（Generalized Linear Model，缩写为 ExpoFamilyGLM）。</p>
<p>对刚刚这类问题如何构建广义线性模型呢？</p>
<p>对于这类问题，我们希望通过一个 $X$ 的函数来预测 $\pmb{y}$ 的值。为了构建出模型，我们先给出3个假设：</p>
<ol>
<li>$y|x;\theta \sim \rm{ExponentialFamily(\eta)}$ 。即，给出 $x$ 和 $\eta$ ，则 $y$ 的分布遵循于指数分布。</li>
<li>给出了 $x$ 我们的目标是预测 $T(y)$ 的期望值。大多数情况下 $T(y) = y$ ，也就是说，我们希望通过假设 $h$ 输出的 $h(x)$ 能满足 $h(x) = E[y|x]$ 。(统计学知识，有点难)  </li>
<li>$\pmb{\eta}$ 和 $\pmb{x}$ 是线性相关的，</li>
</ol>
<h3 id="普通最小二乘（Ordinary-Least-Squares）"><a href="#普通最小二乘（Ordinary-Least-Squares）" class="headerlink" title="普通最小二乘（Ordinary Least Squares）"></a>普通最小二乘（Ordinary Least Squares）</h3><p>普通最小二乘其实是广义线性模型的一个特例，其中 $y$ 是连续的，通过 $x$ 给出的 $y$ 服从高斯分布 $\mathcal{N}(0,\sigma^2)$ ，经过上面的学习我们有：<br>$$<br>\begin{align}<br>h_{\theta}&amp;=E[\pmb{y}|\pmb{x};\pmb{\theta}]\\<br>&amp;=\pmb{\mu}\\<br>&amp;=\pmb{\eta}\\<br>&amp;=\pmb{\theta}^T\pmb{x}.<br>\end{align}<br>$$<br>第一行的等式是基于假设2；第二个等式是基于定理当 $y|x;\theta \sim \mathcal{N}(0,\sigma^2)$ ，则 y 的期望就是 μ；第三个等式是基于假设1，以及之前我们此前将高斯分布写成指数族分布的时候推导出来的性质 $\pmb{\mu}=\pmb{\eta}\$ ；第四个等式就是基于假设3。</p>
<h3 id="Logistic-Regression-1"><a href="#Logistic-Regression-1" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>二分类问题 $y\in {0,1}$ ，可以通过伯努利分布（Bernoulli distribution）来对给定 $x$ 的 $y$ 进行建模。</p>
<p>伯努利分布，有 $\phi=\frac{1}{1+e^{-\eta}}$ ，和在 $ y|x;\theta \sim \rm{Bernoulli}(\phi)$ 下有 $\quad E[y|x;\theta] = \phi$ 。</p>
<p>则有：<br>$$<br>\begin{align}<br>h_{\theta}&amp;=E[\pmb{y}|\pmb{x};\pmb{\theta}]\\<br>&amp;=\pmb{\phi}\\<br>&amp;=\frac{1}{1+e^{-\pmb{\eta}}}\\<br>&amp;=\frac{1}{1+e^{-\pmb{\theta}^T\pmb{x}}}.<br>\end{align}<br>$$<br>这就是为什么在 Logistic Function 中我们用 $\frac{1}{1+e^{-z}}$  做假设，即，一旦我们假设给定 $x$ 的 $y$ 的分布是伯努利分布，那么根据广义线性模型和指数分布族的定义，就会得出这个式子。</p>
<p>注：一个自然参数 $\eta$ 的函数 $g$ ，$g(\eta)=E[T(y)|\eta]$，这个函数叫做规范响应函数（canonical response function），它的反函数 $g^{-1}$ 叫做规范链接函数（canonical link function）。因此，对于高斯分布来说，它的规范响应函数正好就是识别函数（identify function）；而对于伯努利分布来说，它的规范响应函数则是 logistic function。</p>
<h3 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h3><p>对于多分类问题，有 $y\in {1,2,\cdots,k}$ ，通过多项式分布（multinomial distribution） 建模。</p>
<p>把多项式推出一个广义线性模型，首先把多项式分布用指数分布族进行描述。</p>
<p>我们给出  $k$ 个参数 $\phi_1,\cdots,\phi_k$ ，对应各自的输出值的概率，由于 $\sum^k_{i=1}\phi_i=1$ ，所以，有 $\phi_k =1- \sum^{k-1}_{i=1} \phi_i$ 。注意，$\phi_i$ 其实是 $p(u=i;\phi)$ 。现在该出 $T(y)$ ：<br>$$<br>T(1)=\left[\begin{matrix} 1 \\0 \\0 \\ \vdots\\0\end{matrix} \right],<br>T(2)=\left[\begin{matrix} 0 \\1 \\0 \\ \vdots\\0\end{matrix} \right],<br>T(3)=\left[\begin{matrix} 0 \\0 \\ 1 \\ \vdots\\0\end{matrix} \right],\cdots,<br>T(k-1)=\left[\begin{matrix} 0\\0 \\0 \\ \vdots\\1\end{matrix} \right],<br>T(k)=\left[\begin{matrix} 0\\0\\0 \\ \vdots\\0\end{matrix} \right]<br>$$<br>与之前不同，不再有$ T(y) = y$；然后，$T(y)$现在是一个 $k – 1$ 维的向量，而不是一个实数了。向量 $T(y)$ 中的第 i 个元素写成$(T(y))_i$ 。</p>
<p>给出一个记号：指示函数（indicator function），即 $1{\cdot}$ 。如果参数为真，则等于1；反之则等于0。</p>
<p>所以我们可以把 $T (y)$ 和 $y$ 的关系写成 $ (T(y))_i = 1{y = i}$。</p>
<p>现在把多项式写出指数分布族：</p>
<p>$$<br>\begin{align}<br>p(y;\theta) &amp;= \phi^{1\{y=1\}}_{1} \phi^{1\{y=2\}}_{2} \cdots \phi^{1\{y=k\}}_{k}\\<br>&amp;= \phi^{1\{y=1\}}_{1} \phi^{1\{y=2\}}_{2} \cdots \phi^{1-\sum^{k-1}_{i=1}1\{y=i\}}_{k}\\<br>&amp;= \phi^{(T(y))_1}_{1} \phi^{(T(y))_2}_{2} \cdots  \phi^{1-\sum^{k-1}_{i=1}(T(y))_i}_{k}\\<br>&amp;=\rm{exp}\left((T(y))_1\log{(\phi_1)}+(T(y))_2\log{(\phi_2)} + \cdots + \left(1-\sum^{k-1}_{i=1}(T(y))_i\right)\log{(\phi_k)}\right)\\<br>&amp;=\rm{exp}\left((T(y))_1\log{(\phi_1/\phi_k)}+(T(y))_2\log{(\phi_2/\phi_k)} + \cdots + (T(y))_{k-1}\log{(\phi_{k-1}/\phi_k)} +\log{(\phi_k)}\right)\\<br>&amp;=b(y)\rm{exp}(\eta^T\it{T}\rm{}\,(y)-a(\eta))<br>\end{align}<br>$$</p>
<p>其中:</p>
<p>$$<br>\begin{align}<br>\eta&amp;= \left[\begin{matrix} \log{(\phi_1/\phi_k)}\\log{(\phi_2/\phi_k)}\ \vdots\\log{(\phi_{k-1}/\phi_k)}\end{matrix} \right]\\<br>a(\eta) &amp;= -\log{\frac{\phi_i}{\phi_k}}\\<br>b(y) &amp;= 1<br>\end{align}<br>$$</p>
<p>于是对于每一个 $\eta_i$ 有链接函数：</p>
<p>$$<br>\eta_i = \log{(\frac{\phi_i}{\phi_k})}<br>$$</p>
<p>为了简单计算，我们给出定义 $\eta_i = \log{(\phi_k/\phi_k)} = 0$ 。且对链接函数取反函数然后推导出响应函数，就得到了下面的等式：<br>$$<br>\begin{align}<br>e^{\eta_i} &amp; = \frac{\phi_i}{\phi_k}\\<br>\phi_{k}e^{\eta_i} &amp;= \phi_i\\<br>\phi_{k}\sum^k_{i=1}e^{\eta_i}&amp;=\sum^k_{i=1}\phi_i = 1<br>\end{align}<br>$$<br>这样得到 $\phi_k = 1/\sum^k_{i=1}e^{\eta_i}$ ，然后我们我们回代入 $e^{\eta_i} = \frac{\phi_i}{\phi_k}$ ，</p>
<p>得到相应函数：<br>$$<br>\phi_i = \frac{e^{\eta_i}}{\sum^k_{i=1}e^{\eta_i}}<br>$$<br>上面这个函数从 $\eta$ 映射到了$\phi$ ，称为 Softmax函数。通过假设3，我们有了 $\eta_i =\theta_i^Tx $ ，其中的$\theta_1,\theta_2, \dots ,\theta_{k-1} \in \mathbb{R}^{n+1}$ 就是参数了。我们这里还是定义 $\theta_k=0$ ，这样就有 $\eta_k = \theta_k^T x = 0$ ，与上文相呼应。</p>
<p>因此，我们有了模型：<br>$$<br>\begin{align}<br>p(y=i|x;\theta) &amp;=\phi_i\\<br>&amp;=\frac{e^{\eta_i}}{\sum^k_{i=1}e^{\eta_i}}\\<br>&amp;=\frac{e^{\theta_i^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}}<br>\end{align}<br>$$</p>
<p>于是，我们的假设函数是：</p>
<p>$$<br>\begin{align}<br>h_\theta(x)&amp;=E[T(y)|x;\theta]\\<br>&amp;=E\left[\begin{array}{c|c}1\{y = 1\} &amp;\\1\{y = 2\}\\1\{y = 3\}&amp; x;\theta\\ \vdots\\1\{y = k-1\}\end{array}\right]\\<br>&amp;=E\left[\begin{array}{c}\phi_1&amp;\\\phi_2\\\phi_3\\ \vdots\\\phi_{k-1}\end{array}\right]\\<br>&amp;=E\left[\begin{array}{c}<br>\frac{\rm{exp}^{\theta_1^Tx}}{\sum^k_{j=1}\rm{exp}^{\theta_j^Tx}}&amp;\\<br>\frac{\rm{exp}^{\theta_2^Tx}}{\sum^k_{j=1}\rm{exp}^{\theta_j^Tx}}\\<br>\frac{\rm{exp}^{\theta_3^Tx}}{\sum^k_{j=1}\rm{exp}^{\theta_j^Tx}}\\\vdots\\<br>\frac{\rm{exp}^{\theta_{k-1}^Tx}}{\sum^k_{j=1}\rm{exp}^{\theta_j^Tx}}\end{array}\right]<br>\end{align}<br>$$</p>
<p>然后，对于一个训练集来说，我们为了求得 $\pmb{\theta}$ ，写出似然函数的对数：</p>
<p>$$<br>\begin{align}<br>\ell(\pmb{\theta}) &amp;= \sum^m_{i=1}\log{p(y^{(i)}|x^{(i)};\theta)}\\<br>&amp;=\sum^m_{i=1}\log{\prod^k_{l=1}\left(\frac{\rm{exp}^{\theta_{l}^Tx^{(i)}}}{\sum^k_{j=1}\rm{exp}^{\theta_j^Tx^{(i)}}}\right)^{1{y^{(i)}=l}}}<br>\end{align}<br>$$<br>然后我们可以通过梯度上升法或者牛顿法为求：<br>$$<br>\DeclareMathOperator*{\argmax}{arg\,max}<br>\pmb{\theta} = \mathop{\argmax}_{}{\ell(\theta)}<br>$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/Liner-Regressionmd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/Liner-Regressionmd/" itemprop="url">
                  Liner Regression
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:48:09 / 修改时间：16:03:20" itemprop="dateCreated datePublished" datetime="2018-07-03T14:48:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。<br>机器学习中，我们通过线性回归得到的模型对其它的输入值预测出相对应的输出值。上述的模型我们称它为 假设，函数表达为 $h(x):\mathcal{X}\mapsto\mathcal{Y}$</p>
<h2 id="基本过程"><a href="#基本过程" class="headerlink" title="基本过程"></a>基本过程</h2><p>现在，我们有 $m$ 个样本 $\pmb{x}$,对于每一个样本 $\pmb{x}^{(i)}$ ，一共有 $n$ 个特征值，为 $\pmb{x}^{(i)}_{j}$ 。有 $n$ 个系数（模型参数）$\theta_0\quad\theta_1\dots\theta_n$。<br>则我们的预测函数为:</p>
<p>$$<br>h_{\theta}(x) = \theta_0 +\theta_1 x_1+\theta_2 x_2 +\dots+\theta_n x_n<br>$$</p>
<p>为了简单化函数的表示，我们规定 $x_0 = 1$，因此函数中的的参数 $\theta_0$ 就是参数 $\mathcal{bais}$ 。</p>
<p>上面提到的$\mathcal{bais}$，它的的作用会在以后的笔记中详细解释。<br>在计算机中，为了简便计算，我们将数据向量化（vectorize）：<br>$$<br>h(\pmb{x}) = \sum^{n}_{i=0} \theta_i x_i = \pmb{\theta}^{\mathit{T}} \pmb{x},\quad \pmb{\theta} \in \mathbb{R}^{n+1}, \pmb{x} \in \mathbb{R}^{n+1}<br>$$<br>在周志华的《机器学习》一书中，则将$h_{\theta}(\pmb{x})$，表示为：<br>$$<br>f(\pmb{x})=\pmb{w}^T\pmb{x}+b,\quad \pmb{w}\in \mathbb{R}^n,\pmb{x}\in \mathbb{R}^n,<br>$$<br>其实两个函数是一样的，因为有$\theta_0 = b$。但是在计算机中，第一种向量表示比较方便储存数据，我的笔记也就这么记了。</p>
<p>为了使估计值$\hat{y}$接近于$y$，我们定义<strong>cost function</strong> :<br>$$<br>\mathit{J}(\theta) = \frac{1}{2m}\sum ^{m}_{i = 1}(h_{\theta}(x^{(i)})-y^{(i)})^2<br>$$<br>有了cost function，我们的目标就是：</p>
<p>$$<br>\DeclareMathOperator*{\argmin}{arg\,min}<br>\pmb{\theta} = \mathop{\argmin}_{}{\mathit{J}_{\theta}(x)}<br>$$</p>
<h2 id="求解-theta-的方法"><a href="#求解-theta-的方法" class="headerlink" title="求解 $\theta$ 的方法"></a>求解 $\theta$ 的方法</h2><blockquote>
<p>基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。——周志华《机器学习》</p>
</blockquote>
<p>上述的 cost function 就是基于了均方误差。最小二乘法就是试图找到一条“线”，使所有样本到直线的欧式距离之和最小。</p>
<p>求解 $\pmb{\theta}$ 的方法有两个，一个是梯度下降（gradient descent），一个是通过求导得出极值。</p>
<h3 id="梯度下降（gradient-descent）"><a href="#梯度下降（gradient-descent）" class="headerlink" title="梯度下降（gradient descent）"></a>梯度下降（gradient descent）</h3><p>首先，梯度下降是通过不断更新 $\theta$ 的值，而找到最优的情况。对于一个凸函数来说，是让当前的 $\theta $ 往函数下降最快的方向进行移动，以此更新 $\theta $ ，而更新的量，就是函数的导数了。所以，对于某一特征系数有更新公式：<br>$$<br>\theta_j := \theta_j-\alpha \cdot \frac{1}{m} \cdot \frac{\partial}{\partial\theta_j}\mathit{J}(\theta)<br>$$<br>我们从一个样本来看梯度下降，首先对 $\mathit{J}(\theta)$ 求 $\theta$ 的偏导：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\frac{\partial}{\partial\theta_j}\mathit{J}(\theta) &amp;= \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_{\theta}(x)-y)^2\\<br>&amp;=2\cdot\frac{1}{2}(h_{\theta}(x)-y)\cdot \frac{\partial}{\partial\theta_j}(h_{\theta}(x)-y)\\<br>&amp;=(h_{\theta}(x)-y)\cdot \frac{\partial}{\partial\theta_j}(\sum^n_{i=0}\theta_ix_i-y)\\<br>&amp;=(h_{\theta}(x)-y)x_j<br>\end{aligned}<br>\end{equation}<br>$$<br>于是，更新公式可写为：<br>$$<br>\begin{aligned}<br>\theta_j &amp;:= \theta_j-\alpha \cdot(h_{\theta}(x)-y)x_j\\<br>&amp;:=\theta_j +\alpha \cdot(y-h_{\theta}(x))x_j<br>\end{aligned}<br>$$<br>对于一个训练集，我们就得到：<br>$$<br>\begin{align}<br>&amp;\large{重复直到收敛}\{\\<br>&amp; \quad \quad \theta_j := \theta_j +\alpha \cdot \frac{1}{m} \cdot \sum^{m}_{i=1}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_j\\<br> &amp;\}<br>\end{align}<br>$$</p>
<h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><blockquote>
<p>  The “Normal Equation” is a method of finding the optimum theta without iteration.</p>
</blockquote>
<p>首先，我们定义一下 $X$，$X$ 是一个 $m \times n$ 的矩阵，事实上，考虑截距（$\theta_0$），矩阵应该为  $m \times (n+1)$ ，</p>
<p>我们有：<br>$$<br>X = \left[<br> \begin{matrix}<br>-(x^{(1)})^T-\\<br>-(x^{(2)})^T- \\<br>\vdots\\<br>-(x^{(m)})^T-<br>  \end{matrix}<br>  \right]<br>$$<br>然后有 $\pmb{y}$  ：<br>$$<br>\pmb{y} = \left[<br> \begin{matrix}<br>y^{(1)}\\<br>y^{(2)}\\<br>\vdots\\<br>y^{(m})<br>  \end{matrix}<br>  \right],\pmb{y} \in \mathbb{R}^m<br>$$<br>于是有：<br>$$<br>J(\pmb{\theta}) = \frac{1}{2}(X\pmb{\theta}-\pmb{y})^T(X\pmb{\theta}-\pmb{y})<br>$$<br>因此，对 $\mathit{J}(\theta)$ 求 $\theta$ 的偏导：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\pmb{\theta}) &amp;= \nabla_{\theta} \frac{1}{2}(X\pmb{\theta}-\pmb{y})^T(X\pmb{\theta}-\pmb{y})\\<br>&amp; = \frac{1}{2}\nabla_{\theta} (\pmb{\theta}^TX^TX\pmb{\theta}-\pmb{\theta}^TX^T\pmb{y}-\pmb{y}^TX\pmb{\theta}+\pmb{y}^T\pmb{y})\\<br>&amp; =\frac{1}{2}\nabla_{\theta} tr (\pmb{\theta}^TX^TX\pmb{\theta}-\pmb{\theta}^TX^T\pmb{y}-\pmb{y}^TX\pmb{\theta}+\pmb{y}^T\pmb{y})\\<br>&amp; =  \frac{1}{2}\nabla_{\theta}(tr \pmb{\theta}^TX^TX\pmb{\theta} - 2 tr \pmb{y}^TX\pmb{\theta})\\<br>&amp;=\frac{1}{2} (X^TX\pmb{\theta}+X^TX\pmb{\theta}-2X^T\pmb{y})\\<br>&amp;=X^TX\pmb{\theta}-X^T\pmb{y}<br>\end{aligned}<br>$$<br>当对 $\theta$ 的偏导为 $0$ 时，可得极值，得：<br>$$<br>X^TX\pmb{\theta} = X^T\pmb{y}<br>$$<br>得到最优 $\theta$ 解 ：<br>$$<br>\pmb{\theta} = (X^TX)^{-1}X^T\pmb{y}<br>$$</p>
<h2 id="概率解释（Probabilistic-Interpretation）"><a href="#概率解释（Probabilistic-Interpretation）" class="headerlink" title="概率解释（Probabilistic Interpretation）"></a>概率解释（Probabilistic Interpretation）</h2><h3 id="简单说明"><a href="#简单说明" class="headerlink" title="简单说明"></a>简单说明</h3><p>在对数据进行概率假设的基础上，最小二乘回归得到的 $\theta$ 和最大似然法估计的 $\theta$ 是一致的。所以这是一系列的假设，其前提是认为最小二乘回归（least-squares regression）能够被判定为一种非常自然的方法，这种方法正好就进行了最大似然估计（maximum likelihood estimation）。</p>
<h3 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h3><p>首先假设目标变量与输入变量存在以下等量关系：<br>$$<br>\pmb{y}^{(i)}  = \pmb{\theta}^T\pmb{x}+ \pmb{\varepsilon}^{(i)}<br>$$<br>上式的 $\pmb{\varepsilon}^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果 (比如可能某些特征对于房价的影响很明显，但我们做回归的时候忽略掉了)或者随机的噪音信息（random noise）。进一步假设 $\pmb{\varepsilon}^{(i)}$ 是独立同分布的 (IID ，independently and identically distributed) ，服从高斯分布（Gaussian distribution），其平均值为 $0$，方差（variance）为 $\sigma^2$。这样就可以把这个假设写成 $\pmb{\varepsilon}^{(i)} \sim \mathcal{N}(0,\sigma^2)$。然后 $\pmb{\varepsilon}^{(i)}$ 的密度函数就是：<br>$$<br>p(  \pmb{\varepsilon}^{(i)} ) = \frac{1}{\sqrt{2\pi}\sigma}\mathrm{exp}(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})<br>$$<br>这意味着存在下面的等量关系：<br>$$<br>p(\pmb{y}^{(i)}|\pmb{x}^{(i)};\pmb{\theta }) = \frac{1}{\sqrt{2\pi}\sigma}\mathrm{exp}(-\frac{(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2}{2\sigma^2})<br>$$<br>上式为，在 $\theta$ 取某个固定值的情况下，这个等式表达为，在 $\pmb{x}^{(i)}$ 的情况下发生 $\pmb{y}^{(i)}$ 的概率， 通常可以看做是一个 $\pmb{y}^{(i)}$ 的函数。当我们要把它当做 $\theta$ 的函数的时候，就称它为似然函数（likelihood function）在整个数据集下有：<br>$$<br>L(\pmb{\theta}) = L(\pmb{\theta};X,\pmb{y}) = p(\pmb{y}|X;\pmb{\theta })<br>$$<br>结合之前对 $ \pmb{\varepsilon}^{(i)}$ 的独立性假设（这里对 $\pmb{y}^{(i)}$ 以及给定的 $\pmb{x}^{(i)}$ 也都做同样假设），就可以把上面这个等式改写成下面的形式：<br>$$<br>\begin{align}<br>L(\pmb{\theta}) &amp;= \prod^m_{i=1}(\pmb{y}^{(i)}|\pmb{x}^{(i)};\pmb{\theta }) \\<br>&amp;= \prod^m_{i=1} \frac{1}{\sqrt{2\pi}\sigma}\mathrm{exp}(-\frac{(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2}{2\sigma^2})<br>\end{align}<br>$$<br>最大似然法（maximum likelihood）告诉我们要选择能让数据的似然函数尽可能大的 $\theta$ 。也就是说，找到  $\theta$ 能够让函数 $L(\pmb{\theta})$ 取到最大值。</p>
<p>为了找到 $L(\pmb{\theta})$ 的最大值，我们不能直接使用 $L(\pmb{\theta})$ ，而要使用严格递增的 $L(\pmb{\theta})$ 的函数求最大值。使用对数函数来找对数函数 $L(\pmb{\theta})$ 的最大值是一种方法，而且求导来说就简单了一些：<br>$$<br>\begin{align}<br>v&amp; = \log{\mit{L}(\pmb{\theta})}\\<br>&amp;=\log{ \prod^m_{i=1} \frac{1}{\sqrt{2\pi}\sigma}\mathrm{exp}(-\frac{(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2}{2\sigma^2})}\\<br>&amp;=\sum^m_{i=1}\log{ \prod^m_{i=1} \frac{1}{\sqrt{2\pi}\sigma}\mathrm{exp}(-\frac{(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2}{2\sigma^2})}\\<br>&amp;=m\log{ \frac{1}{\sqrt{2\pi}\sigma} -\frac{1}{\sigma^2} \cdot \sum^m_{i=1}(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2}<br>\end{align}<br>$$<br>对于上式 ，由于$m\log{\frac{1}{\sqrt{2\pi}\sigma}}$ ，$\frac{1}{\sigma^2}$ 值不变，那么 $\scr{L}(\pmb{\theta})$ 取最大，即求下面的式子最小：<br>$$<br>\frac{1}{2}\sum^m_{i=1}(y^{(i)}-\pmb{\theta}^T\pmb{x}^{(i)})^2<br>$$<br>证毕。</p>
<h2 id="多项式回归（Polynomial-Regession）"><a href="#多项式回归（Polynomial-Regession）" class="headerlink" title="多项式回归（Polynomial Regession）"></a>多项式回归（Polynomial Regession）</h2><p>多项式回归可以用来拟合二次、三次、高次模型，通过使用 $\pmb{x}^2,\sqrt{\pmb{x}}$ 等进行拟合。 </p>
<p>这样便将高阶方程模型转换成线性回归模型。这也算是 <strong>特征缩放(Features Scaling)</strong> 的一种。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/Symbolic-Representation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/Symbolic-Representation/" itemprop="url">
                  Symbolic Representation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:47:09 / 修改时间：16:08:23" itemprop="dateCreated datePublished" datetime="2018-07-03T14:47:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine-Learning</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>$x$  ：一个标量</p>
<p>$\pmb{x}_i$ ：一个样本 </p>
<p>$\pmb{x}^i$ ：样本的某一特征</p>
<p>$D$ ：数据集合</p>
<p>$\mathcal{X}$ ：样本空间</p>
<p>$\mathcal{Y}$ ：“标记空间” 或 “输出空间”</p>
<p>$n$ ：特征数</p>
<p>$m$ ：样本数</p>
<p>$\mathcal{D}$ ：概率分布</p>
<p>$\alpha$ ：学习率</p>
<p>$\mu$ ：$D$上某一特征下$\pmb{x}^i$的均值</p>
<p>$\sigma$ ：$D$上某一特征下$\pmb{x}^i$的标准差</p>
<p>$\mathcal{H}​$ ：假设空间</p>
<p>$\widehat{y}$ ：$y$ 的估计量</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/求极限-题一/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/求极限-题一/" itemprop="url">
                  求极限 题一
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:42:09 / 修改时间：15:34:54" itemprop="dateCreated datePublished" datetime="2018-07-03T14:42:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/考研/" itemprop="url" rel="index"><span itemprop="name">考研</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>求 $$\lim_{x \to 0}\frac{\mit{e}^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{\mit{e}}{x}}}{x^2}$$</p>
<p>解答：<br>$$<br>\begin{align}<br>记：&amp;y =\frac{ln{(1+x)}}{x}\\<br>有：&amp;\lim_{x \to 0} y = 1\\<br>&amp;y’ = -\frac{ln(1+x)}{x^2}+\frac{1}{x(1+x)}\\<br>则：&amp;\lim_{x \to 0}\frac{dy}{dx}= \lim_{x \to 0} y’=-\frac{1}{2}\\<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>所以：原式&amp;=\lim_{x \to 0}\frac{\mit{e}^{\mit{e}^y}-\mit{e}^{\mit{e}y}}{x^2}\\<br>&amp;=\lim_{x \to 0}\frac{\mit{e}^{\mit{e}^y+y}-\mit{e}^{\mit{e}y+1}}{2x} \cdot \frac{dy}{dx}\\<br>&amp;=\lim_{x \to 0}\frac{\mit{e}^{\mit{e}y+1}-\mit{e}^{\mit{e}^y+y}}{4x}\\<br>&amp;=\lim_{x \to 0}\frac{\mit{e}^{\mit{e}y+1}\mit{e}\frac{dy}{dx}-\mit{e}^{\mit{e}^y+y}(\mit{e}^{y}\frac{dy}{dx}+\frac{dy}{dx})}{4}\\<br>&amp;=\lim_{x \to 0}\frac{\mit{e}^{\mit{e}y+1}\mit{e}\cdot (\frac{-1}{2})-\mit{e}^{\mit{e}^y+y}(\mit{e}^{y}\cdot (\frac{-1}{2})+\frac{-1}{2})}{4}\\<br>&amp; = \lim_{y \to 1}\frac{\mit{e}^{\mit{e}+1} \cdot \mit{e} \cdot (\frac{-1}{2})-\mit{e}^{\mit{e}+1} \cdot \mit{e} \cdot (\frac{-1}{2})-\mit{e}^{\mit{e}+1} \cdot (\frac{-1}{2})}{4}\\<br>&amp;= \frac{\mit{e}^{\mit{e}+1}}{8}<br>\end{align}<br>$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/隐函数存在定理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/隐函数存在定理/" itemprop="url">
                  隐函数存在定理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:26:09 / 修改时间：14:29:43" itemprop="dateCreated datePublished" datetime="2018-07-03T14:26:09+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/考研/" itemprop="url" rel="index"><span itemprop="name">考研</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/media/15305992039707.jpg" alt="-w371"></p>
<p>$F(x_0,y_0)=0$,<br>$d_x$就是在沿x轴的斜率，$d_y$就是沿y轴的斜率。<br>$F’_x$为沿x轴的增量，$F’_y$为沿y轴的增量，$d_z$为沿z轴的增量。<br>则：<br>$$\frac{d_y}{d_x}=-\frac{tan \theta_1}{tan{\theta_2}}=-\frac{\frac{d_z}{F’_y}}{\frac{d_z}{F’_x}}=-\frac{F’_x}{F’_y}$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.afuture.me/2018/07/03/ACM数学知识整理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andy Francis">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AFuture's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/03/ACM数学知识整理/" itemprop="url">
                  ACM数学知识整理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-07-03 14:22:46 / 修改时间：14:29:44" itemprop="dateCreated datePublished" datetime="2018-07-03T14:22:46+08:00">2018-07-03</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>数学</em></p>
<h2 id="数论"><a href="#数论" class="headerlink" title="数论"></a>数论</h2><h3 id="素数，整数分解，欧拉函数"><a href="#素数，整数分解，欧拉函数" class="headerlink" title="素数，整数分解，欧拉函数"></a>素数，整数分解，欧拉函数</h3><pre><code>辗转相除法
筛法：  
    pku2689 Prime Distance（很好的一个应用）  
反素数：  
    zoj2562 More Divisors  
素数判断，整数分解：  
    pku1811 Prime Test  
    pku2429 GCD &amp; LCM Inverse  
欧拉函数：  
    pku1284 Primitive Roots （很水）  
    pku2407 Relatives （很水）  
    pku2773 Happy 2006  
    pku2478 Farey Sequence （快速求欧拉函数）  
    pku3090 Visible Lattice Points （法雷级数）  
推荐：（欧拉函数，费马小定理）  
    pku3358 Period of an Infinite Binary Expansion 
整数分解  
    pku2992 Divisors  
    fzu1753 Another Easy Problem   
    hit2813 Garden visiting 
    pku3101 Astronomy （分数的最小公倍数） 
</code></pre><h3 id="扩展欧几里得，线性同余，中国剩余定理"><a href="#扩展欧几里得，线性同余，中国剩余定理" class="headerlink" title="扩展欧几里得，线性同余，中国剩余定理"></a>扩展欧几里得，线性同余，中国剩余定理</h3><pre><code>简单题：  
    pku1006 Biorhythms  
    pku1061 青蛙的约会
    pku2891 Strange Way to Express Integers  
    pku2115 C Looooops  
    pku2142 The Balance  
    *强烈推荐：  
    pku3708 Recurrent Function （经典）
</code></pre><h3 id="高次同余方程"><a href="#高次同余方程" class="headerlink" title="高次同余方程"></a>高次同余方程</h3><pre><code>pku3243 Clever Y    
pku2417 Discrete Loggin
</code></pre><h3 id="高斯消元法解方程"><a href="#高斯消元法解方程" class="headerlink" title="高斯消元法解方程"></a>高斯消元法解方程</h3><pre><code>简单题：  
    pku1222 EXTENDED LIGHTS OUT    
    pku1681 Painter&apos;s Problem    
    pku1830 开关问题    
推荐：  
    pku2947 Widget Factory    
    pku2065 SETI    
强烈推荐：  
    pku1753 Flip Game    
    pku3185 The Water Bowls    
变态题：  
    pku1487 Single-Player Games 
</code></pre><h3 id="数位类统计问题"><a href="#数位类统计问题" class="headerlink" title="数位类统计问题"></a>数位类统计问题</h3><pre><code>简单：  
    ural1057 Amount of degrees
    spoj1182 Sorted bit squence 
    hdu3271 SNIBB    
较难：  
    spoj2319 Sequence  
    sgu390 Tickets 
</code></pre><h2 id="组合数学"><a href="#组合数学" class="headerlink" title="组合数学"></a>组合数学</h2><h3 id="容斥原理，鸽巢原理"><a href="#容斥原理，鸽巢原理" class="headerlink" title="容斥原理，鸽巢原理"></a>容斥原理，鸽巢原理</h3><pre><code>鸽巢原理：  
    pku2365 Find a multiple    
    pku3370 Halloween treats    
容斥原理：  
    hdu1695 GCD    
    hdu2461 Rectangles  
</code></pre><h3 id="排列组合，区间计数，计数序列"><a href="#排列组合，区间计数，计数序列" class="headerlink" title="排列组合，区间计数，计数序列"></a>排列组合，区间计数，计数序列</h3><pre><code>简单题：  
    pku1850 Code    
    pku1150 The Last Non-zero Digit    
    pku1715 Hexadecimal Numbers    
    pku2282 The Counting Problem    
    pku3286 How many 0&apos;s?    
推荐：  
    pku3252 Round Numbers    
计数序列：  
    pku1430 Binary Stirling Numbers    
    pku2515 Birthday Cake    
    pku1707 Sum of powers    
</code></pre><h3 id="置换，置换的运算"><a href="#置换，置换的运算" class="headerlink" title="置换，置换的运算"></a>置换，置换的运算</h3><pre><code>简单题：（应该理解概念就可以了）  
    pku3270 Cow Sorting  
    pku1026 Cipher 
置换幂运算：  
    pku1721 CARDS  
    pku3128 Leonardo&apos;s Notebook            
推荐：（不错的应用）  
    pku3590 The shuffle Problem 
</code></pre><h3 id="burnside定理，polya计数法"><a href="#burnside定理，polya计数法" class="headerlink" title="burnside定理，polya计数法"></a>burnside定理，polya计数法</h3><pre><code>简单题：（直接用套公式就可以了)
    pku2409 Let it Bead
    pku2154 Color
    pku1286 Necklace of Beads
强烈推荐：（这题很不错哦，很巧妙）  
    pku2888 Magic Bracelet
</code></pre><h3 id="稳定婚姻问题"><a href="#稳定婚姻问题" class="headerlink" title="稳定婚姻问题"></a>稳定婚姻问题</h3><pre><code>pku3487 The Stable Marriage Problem    
zoj1576 Marriage is Stable 
</code></pre><h3 id="MoBius反演"><a href="#MoBius反演" class="headerlink" title="MoBius反演"></a>MoBius反演</h3><h3 id="偏序关系理论"><a href="#偏序关系理论" class="headerlink" title="偏序关系理论"></a>偏序关系理论</h3><h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><pre><code>简单：  
    pku3070 Fibonacci    
    pku3233 Matrix Power Series    
    pku3735 Training little cats 
</code></pre><h3 id="矩阵变换"><a href="#矩阵变换" class="headerlink" title="矩阵变换"></a>矩阵变换</h3><h3 id="递推关系"><a href="#递推关系" class="headerlink" title="递推关系"></a>递推关系</h3><pre><code>POJ3252
poj1850
poj1019
poj1942
</code></pre><h3 id="约瑟夫环问题"><a href="#约瑟夫环问题" class="headerlink" title="约瑟夫环问题"></a>约瑟夫环问题</h3><pre><code>简单题：  
    pku3517 And Then There Was One  
    pku1781 In Danger  
    pku1012 Joseph   
    pku2244 Eeny Meeny Moo   
</code></pre><h3 id="找规律，推公式"><a href="#找规律，推公式" class="headerlink" title="找规律，推公式"></a>找规律，推公式</h3><pre><code>pku3372 Candy Distribution    
pku3244 Difference between Triplets    
pku1809 Regetni    
pku1831 不定方程组    
pku1737 Connected Graph    
pku2480 Longge&apos;s problem    
pku1792 Hexagonal Routes
</code></pre><h3 id="二分法求解单调函数相关知识"><a href="#二分法求解单调函数相关知识" class="headerlink" title="二分法求解单调函数相关知识."></a>二分法求解单调函数相关知识.</h3><pre><code>pku3273Monthly Expense 
pku3258River Hopscotch  
pku1905Expanding Rods 
pku3122Pie 
pku1845 Sumdiv 
poj3273
poj3258
poj1905
poj3122
</code></pre><h3 id="三分法求解单峰-单谷-的极值"><a href="#三分法求解单峰-单谷-的极值" class="headerlink" title="三分法求解单峰(单谷)的极值."></a>三分法求解单峰(单谷)的极值.</h3><h3 id="0-1分数规划-poj2976"><a href="#0-1分数规划-poj2976" class="headerlink" title="0/1分数规划. (poj2976)"></a>0/1分数规划. (poj2976)</h3><h3 id="矩阵法"><a href="#矩阵法" class="headerlink" title="矩阵法"></a>矩阵法</h3><h3 id="随机化算法"><a href="#随机化算法" class="headerlink" title="随机化算法"></a>随机化算法</h3><pre><code>poj3318
poj2454
</code></pre><h3 id="迭代逼近"><a href="#迭代逼近" class="headerlink" title="迭代逼近"></a>迭代逼近</h3><pre><code>poj3301
</code></pre><h3 id="扫描线算法"><a href="#扫描线算法" class="headerlink" title="扫描线算法"></a>扫描线算法</h3><h2 id="博弈论"><a href="#博弈论" class="headerlink" title="博弈论"></a>博弈论</h2><h3 id="巴什博奕（BashGame）"><a href="#巴什博奕（BashGame）" class="headerlink" title="巴什博奕（BashGame）"></a>巴什博奕（BashGame）</h3><h3 id="威佐夫博奕（WythoffGame）"><a href="#威佐夫博奕（WythoffGame）" class="headerlink" title="威佐夫博奕（WythoffGame）"></a>威佐夫博奕（WythoffGame）</h3><h3 id="尼姆博奕（NimmGame）"><a href="#尼姆博奕（NimmGame）" class="headerlink" title="尼姆博奕（NimmGame）"></a>尼姆博奕（NimmGame）</h3><h3 id="组合博弈"><a href="#组合博弈" class="headerlink" title="组合博弈"></a>组合博弈</h3><h2 id="计算几何"><a href="#计算几何" class="headerlink" title="计算几何"></a>计算几何</h2><h3 id="叉积和点积的运用-如线段相交的判定-点到线段的距离等"><a href="#叉积和点积的运用-如线段相交的判定-点到线段的距离等" class="headerlink" title="叉积和点积的运用(如线段相交的判定,点到线段的距离等)."></a>叉积和点积的运用(如线段相交的判定,点到线段的距离等).</h3><pre><code>poj2031
poj1039
</code></pre><h3 id="多边型的简单算法-求面积-和相关判定-点在多边型内-多边型是否相交"><a href="#多边型的简单算法-求面积-和相关判定-点在多边型内-多边型是否相交" class="headerlink" title="多边型的简单算法(求面积)和相关判定(点在多边型内,多边型是否相交)"></a>多边型的简单算法(求面积)和相关判定(点在多边型内,多边型是否相交)</h3><pre><code>poj1408
poj1584
</code></pre><h3 id="二维、三维凸包"><a href="#二维、三维凸包" class="headerlink" title="二维、三维凸包"></a>二维、三维凸包</h3><pre><code>poj2187
poj1113
</code></pre><h3 id="扫描线算法-例如求矩形的面积和周长并-常和线段树或堆一起使用"><a href="#扫描线算法-例如求矩形的面积和周长并-常和线段树或堆一起使用" class="headerlink" title="扫描线算法(例如求矩形的面积和周长并,常和线段树或堆一起使用)."></a>扫描线算法(例如求矩形的面积和周长并,常和线段树或堆一起使用).</h3><h3 id="半平面求交"><a href="#半平面求交" class="headerlink" title="半平面求交"></a>半平面求交</h3><pre><code>poj3384
poj2540
poj3130
poj3335
</code></pre><h3 id="点集最小圆覆盖"><a href="#点集最小圆覆盖" class="headerlink" title="点集最小圆覆盖."></a>点集最小圆覆盖.</h3><h3 id="对踵点（最远点对）"><a href="#对踵点（最远点对）" class="headerlink" title="对踵点（最远点对）"></a>对踵点（最远点对）</h3><pre><code>poj2079
</code></pre><h3 id="几何工具的综合应用"><a href="#几何工具的综合应用" class="headerlink" title="几何工具的综合应用."></a>几何工具的综合应用.</h3><pre><code>poj1819
poj1066
poj2043
poj3227
poj2165
poj3429
</code></pre>
          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Andy Francis</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">32</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andy Francis</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	





  





  










  





  

  

  

  
  

  
  
    
      
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
